# Chapter 14 가중치의 진짜 업데이트

하지만 신경망에서 노드는 단순한 선형 분류자가 아닙니다.

### 가중치 계산

신경만에는 너무나도 많은 가중치의 조합이 존재.

각 계층의 출력이 다음 계층의 입력과 관련되어
너무 복잡하게 되므로 한 방에 풀어주는 대수학을 활용할 수 없음.


### 경사 하강법

현실적인 부분을 받아들이자.

- 수식으로 표현하면 너무 복잡함 & 조합이 너무 많음

- 학습 데이터가 충분하지 않거나, 학습 데이터 자체에 오류가 있을 경우.


경사하강법: 기울기를 활용하여 최저점을 찾아가는 방법


```
중간 정리
- 경사 하강법은 함수의 최저점을 구하기 위한 좋은 접근 방법입니다.
특히 함수가 매우 복잡하고 어려워 대수학을 이용해 수학적 접근 방식으로 풀기 어려울 때도 잘 동작합니다.
- 매개변수가 많아서 다른 접근 방법들이 실패하거나 현실적이지 못한 경우에도 경사 하강법은 잘 동작합니다.
- 데이터가 불완전하거나 함수가 완벽하게 표현되지  못하거나 잘못된 발검음을 내디딘 경우에도 경사 하강법은 탄력적으로 대응합니다.
```

이후 과정은 다시 봐야할 것 같다.
흐름은 이해하였으나, 수식과 개념의 연결이 잘 안 된다.


### 여러 가지 오차함수

- 목표값 - 실제값
- |목표값 - 실제값|
- (목표값 - 실제값) ^ 2


### 미분으로 오차함수 구하기

경사하강법과 오차함수를 통해 목표값을 찾아가는 과정을 진행.

미분을 통해 기울기가 작을수록 목표값에 근접.

이후는 수식...


### 학습률

새 가중치 = 이전 가중치 - alpha * 오차 기울기



### 핵심 정리

- 신경망의 오차는 가중치의 함수입니다.
- 신경망을 개선한다는 것은 가중치의 변화를 통해 오차를 줄인다는 뜻입니다.
- 최적의 가중치를 직접 찾는 것은 매우 어렵습니다. 이를 대체하는 접근 방법은 작은 발걸음으로 오차함수를 줄여가면서 반복적으로 가중치를 개선해가는 방법입니다. 각 발걸음은 현재 위치에서 볼 때 가장 급격히 낮아지는 경사의 방향으로 취해집니다. 이러 방법을 경사 하강법이라고 합니다.
- 오차 기울기는 미분을 이용해 계산할 수 있으며 알고 보면 별로 어렵지 않습니다.

